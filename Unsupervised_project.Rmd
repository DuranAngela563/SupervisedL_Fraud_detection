---
title: "PROJECT 1: UNSUPERVISED LEARNING"
subtitle: "Statistical Learning. Bachelor in Data Science and Engineering"
author: "Ángela María Durán Pinto"
date: '05/11/2023'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: true
    toc: true
    toc_depth: 2
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
if (requireNamespace("thematic")) 
  thematic::thematic_rmd(font = "auto")
```

```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

# Main Objective

Have you ever heard: "Money doesn't give happiness"? Well, during this project we are going to answer that question, and we are going to identify groups of countries exhibit similar characteristics or trends in their happiness scores and the associated factors.This can be valuable for policy analysis, international comparisons, and understanding the drivers of well-being in different regions.

# Analyze Dataset

The first thing that we have to do before starting the project, is understand our variables for future decisions.

Our dataset is "The World Happiness Report", which contains a variety of data related to happiness scores, as well as factors that contribute to overall well-being in different countries.

Let's define our variables:

-   Ladder score: Happiness score or subjective well-being. National average response to the question of life evaluations.

-Standard error: indicates the level of uncertainty or margin of error associated with the reported "Ladder score."

-   GDP: Gross Domestic Product

-Healthy Life Expectancy (HLE). They are based on the data extracted from the World Health Organization's (WHO) Global Health Observatory data repository

-Social support (or having someone to count on in times of trouble) is the national average of the binary responses (either 0 or 1) to the GWP question "If you were in trouble, do you have relatives or friends you can count on to help you whenever you need them, or not?" -Freedom to make life choices is the national average of responses to the GWP question

-Generosity: the residual of regressing national average of response to the GWP question "Have you donated money to a charity in the past month?" on GDP per capita.

-Corruption Perception: The measure is the national average of the survey responses to two questions

-Upperwhisker: Positive affect defined as the average of three positive affect measures in GWP: laugh, enjoyment and doing interesting things

-Lowerwhisker: Negative affect defined as the average of three negative affect measures in GWP. They are worry, sadness and anger.

-Positive affect is defined as the average of three positive affect measures in GWP: laugh, enjoyment and doing interesting things in the Gallup World Poll waves 3-7.

-Negative affect is defined as the average of three negative affect measures in GWP. They are worry, sadness and anger,

Load necessary libraries

```{r}

#install.packages("readxl")
library(readxl)

rm(list=ls()) 

library(tidyverse)
library(countrycode)
library(rworldmap)
library(ggplot2)

# Dealing with missing values
library(mice)

#GGally: collection of functions for creating plots and visualizations to explore and understand relationships between variables in your data. : heatmap-like graphs, and ggcorr

library(GGally)
# Visualisation and Imputation of Missing Data

library(VIM)
# extracting and visualizing information from multivariate data analyses, particularly factor analysis and PCA)
library(factoextra)

# for clustering
library(cluster)
library(mclust)

# kernel k-means
library(kernlab)

```

Load dataset

```{r}
data <- read_excel("World_Happiness.xls")

# Let's change the names of the columns as they are long and with spaces
new_column_names <- c("Country", "Score", "StandardError", "Upperwhisker", "Lowerwhisker", "LogGDP", "SocialSupport", "HLE", "Freedom", "Generosity", "PerceptionsCorruption","ScoreDystopia", "ExLogGDP", "ExSocialSupport", "ExHLE", "ExFreedom", "ExGenerosity", "ExPerceptionsCorruption", "DystopiaResidual"   )

colnames(data) <- new_column_names

summary(data)
head(data)
dim(data)
str(data)
tail(data)

```

## Feature Extraction

We want to add to our "data" 2 columns which are goin to be obtained from another source: "additional". In additional we have data for different years for each country, so we get the vale of PositiveAffect and NegativeAffect only for year 2021. We will get some missing values as there some countries in "data" which are not in "additional". We will deal later with missing values.

```{r}
additional <- read_excel("additional.xls")
idx = 1
# Initialize our new columns with NAs
data$PositiveAffect <- NA
data$NegativeAffect <- NA

# for each country
for (i in data$Country) {
  # Compute the index of the row with country name= i and for year 2021
  pos_idx <- which(additional$`Country name` == i & additional$year == 2021)
  
  # Only if there exists the country i in "additional"
  if (length(pos_idx) > 0) {
    # Assign the Positive Affect value if it's found, else keep it as 0
    data$PositiveAffect[idx] <- additional$`Positive affect`[pos_idx]
    data$NegativeAffect[idx] <- additional$`Negative affect`[pos_idx]
  }
  idx = idx + 1
}

```

## Duplicated Observations

Check duplicated observations (lab clustering class)

```{r}

# 1º search for exactly duplicated rows. We obtain 0. This is usual as 1 one the values can change, but we still consider it a duplicated row 
sum(duplicated(data))

# 2º search for observations with duplicated country, as all the other variables are numeric
sum(duplicated(data$Country))  # Obtain 2 duplicated rows
duplicated_rows <- which(duplicated(data$Country))

# We see that Panama and Portugal have a duplicated observation
data$Country[duplicated(data$Country)]

# Remove them
data = data[-duplicated_rows,]

# Another way is using dplyr package to keep only the unique rows based on the "Country" column.
data_cleaned <- data %>%
  distinct(Country, .keep_all = TRUE)

```

```{r}
country_names <- data$Country
```

## Feature Selection

It is important to remember that we should only use continuous variables in our study. The reason is that if the different classes of our categorical variables are unbalanced, the one with more observations is going to take too much importance. We are not taking the variable "Country name". Also, we remove "Standard Error of Ladder Score" as this variable represents the uncertainty associated with the reported "Ladder Score." It doesn't provide direct information about the well-being of a country

Explained by Factors (e.g., Log GDP per capita, Social Support, etc.): These variables represent the portion of the "Ladder Score" explained by specific factors. While they are important for understanding the contributions of individual factors to happiness, they are not typically used for clustering because they already represent components of the "Ladder Score."

Upper Whisker and Lower Whisker: These variables are related to data visualization and do not represent the inherent characteristics of a country's well-being.

```{r}
data2 =  data %>% dplyr::select(-Country, -StandardError, -Upperwhisker, -Lowerwhisker, -ExLogGDP, -ExSocialSupport, -ExHLE, -ExFreedom, -ExGenerosity, -ExPerceptionsCorruption,-DystopiaResidual)


```

We have reduced our considerably our dataset variables: from 19 to 10. The other variables were not useful for the main objective of this work.

## Outliers

An outlier is an observation or data point that significantly differs from other observations in a dataset. It is an unusual or rare value that falls outside the typical range of values in a dataset.

-   1º We make a general boxplot for the entire dataset

```{r}
# Don't forget to scale the data
boxplot(scale(data2), col = "lightblue", las = 2)

```

We can see that variables as "Score", "LogGDP", "lowerwisker may contain outliers". Therefore, we have to look more carefully to those variables an make an individual boxplot

```{r}

par(mfrow=c(1,3))
# For "Score"
ggplot(data) +
  aes(x = "", y = Score) +
  geom_boxplot(fill = "lightblue") +
  labs(y = "Score") +
  theme_light()

# For "Lowerwhisker"
ggplot(data) +
  aes(x = "", y = Lowerwhisker) +
  geom_boxplot(fill = "lightpink") +
  labs(y = "Lowerwhisker") +
  theme_light()

# For "LogGDP"

ggplot(data) +
  aes(x = "", y = LogGDP) +
  geom_boxplot(fill = "lightyellow") +
  labs(y = "LogGDP") +
  theme_light()


```

We see that could be an outlier with score \<2, Lowershisker \> 2, and LogGDP \< 5.5

## Missing values

(lab1 data preprocessing)

Missing values, often denoted as NA (Not Available) or NaN (Not-a-Number), are placeholders used in data to represent the absence of a value or an unknown value for a particular observation or variable. They can have a significant impact on data analysis and modeling.

We can see the distribution of the NAs with the mice package

```{r}
sum(is.na(data2)) ## 31 missing values

md.pattern(data2)
```

```{r}
aggr(data2, number = TRUE, sortVars = TRUE, labels = names(data2),
     cex.axis = .7, gap = 1, ylab= c('Missing data','Pattern'))
```

We can see that we have 1 observation with 3 missing values and 14 observations with 2 missing values. We could think of removing the observation with 3 missing values, but it has more than the half of the values, we are following a different aproach. Additionally, we see that both variables "PositiveAffect" and "NegativeAffect" have 15 NAs

The "mice" package in R is a powerful tool which is an iterative imputation method that replaces missing values with multiple imputations using a regression model. The imputed values are then used to estimate the missing values in the subsequent iteration until the convergence criteria are met. We are going to use Random Forest as method

```{r}
# If we want to remove the observation:   data[-which(rowSums(is.na(data)) ==5,)

imp <- mice(data2, method = 'rf', m = 7)  
data2 = complete(imp)
summary(data2)
sum(is.na(data2))
```

Now, we don't have missing values anymore.

# Visualization

Visualizations are a crucial part of EDA. Effective visualizations help you understand data distributions, trends, and anomalies. Along this section we are going to make some questions and use a plot to answer them.

## Univariate: Dimension 1

We have already seen the boxplots.

**1)** What is the distribution of the "Score"? In this case we are going to generate a histogram that shows the distribution of happiness scores.

```{r}

ggplot(data, aes(x = Score)) +
  geom_histogram(binwidth = 0.39, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Happiness Scores",
       x = "Score",
       y = "Frequency")
```

It is slightly skewed to the right (positively skewed), which means that higher happiness scores are more common. As the histogram is spread, it indicates than Score has variability. See possible outliers socre \< 2

```{r}

sum(which(data2$Score < 2))

# We have 2 observations with a really low score
r <- which(data2$Score < 3)
n <- data$Country[r]
print(n)
```

Now, I think that these observations are not outliers as in "Lebanon" and "Afghanistan" is understandable a low score because of their bad situation.

```{r}
# Create a histogram for the "LogGDP" variable
ggplot(data, aes(x = LogGDP)) +
  geom_histogram(binwidth = 0.5, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Log GDP",
       x = "Log GDP",
       y = "Frequency")
print(data$Country[which(data2$LogGDP < 6)])  # Venezuela

```

```{r}
ggplot(data, aes(y = SocialSupport)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Box Plot of Social Support",
       x = "",
       y = "Social Support")
```

##Dimension 2: bivariate analysis (scatter plots)

As we want to know weather the wealthy a contry is has an impact on the level of happines, we are going to make an scatterplot to visualize the relationship

```{r}
ggplot(data2, aes(x = LogGDP, y = Score)) +
  geom_point(color = "darkblue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Scatterplot of Score vs. LogGDP with Regression Line",
       x = "LogGDP (Logged GDP per capita)",
       y = "Score (Happiness Score)")
```

Without any doubt, we can see a strong positive linear relationship between GDP and Score.

Multiple scatterplots:

```{r}
# When applying ggcorr() -> obtain Warning: the standard deviation is zero. 
# a standard deviation of zero means that one or more variables in our dataset have no variability because all of # their values are the same. So, we check this
vars_0sd = colnames(data2)[apply(data2, 2, sd) == 0]  
# We realise that "ScoreDystopia" takes always the value 1.777825. Therefore,I am going to exclude it as it won't provide meaningful correlation values.

data2 =  data2 %>% dplyr::select(-ScoreDystopia)

ggcorr(data2, label = T)
heatmap(cor(data2))

```

We can see that the hapiness score is highly positive related with GDP and SocialSupport, and negative related with PerceptionsCorruption and NegativeAffect. These are expected results. However, I find it surprising that Score and Generosity are independent.

Let's see the relation between each pair of our variables

```{r}
pairs(data2, pch = 19, col = "lightblue")
 
```

# PCA

(lab PCA)

Principal Component Analysis (PCA) is a widely used statistical technique for dimensionality reduction and data transformation. PCA is employed when dealing with high-dimensional data, where each observation is described by multiple variables. It allows to identify and highlight the most important patterns or directions in the data. It creates new variables (principal components) that are linear combinations of the original variables, capturing the maximum variance in the data.

It is important to remember that PCA is sensitive to the scaling of the data. When variables have different scales, those with larger scales will contribute more to the overall variance. This can lead to the PCA being dominated by variables with large scales, while variables with smaller scales may have limited influence on the principal components.

```{r}
pca = prcomp(data2, scale=T)
# pca = princomp(nba, cor=T) # the same, but using SVD instead of eigen decomposition 
summary(pca)

```

We see that PC1 explains 52.9% of the total variance, it is the most important component. PC2 is the second component which explains more variance, 15.6%. PC3 explains 9.76%. With these 3 component we can explain 78.26% of the variance. But, is this enough or should we take more components?

## Number of PCA components

We are using **screeplot** to generate a PCA scree plot that visually displays the explained variance for each principal component. This visualization aids in pinpointing the 'elbow point,' where the explained variance curve starts to stabilize, providing insights into the dimensionality of the data.

For that, we use fviz_screeplot() function from factoextra package:

```{r}
fviz_screeplot(pca, addlabels = TRUE)
```

We decide to take 3 components.

## Interpretation of the Components

Interpreting the components in a Principal Component Analysis (PCA) involves understanding the relationship between the original variables and the principal components, which are linear combinations of the original variables.

\###**1º Component**

We are creating a bar plot for the first principal component's loadings in a PCA analysis. We need to extract the loadings which represent the strength and direction of each original variable's contribution to the first component.

First, we are going to plot the **square loadings** which indicate the proportion of variance in each original variable that is explained by the associated principal component. Higher squared loadings indicate that the variable contributes more to that component.

```{r}
fviz_contrib(pca, choice = "var", axes = 1)
```

We see that "Score" is the one which most contributes as expected. On the other hand "Generosity" is the one which contributes less as it was independent to Score.

Now, let's analyze the Direction of Component Loadings. For that, we need to examine the signs of the component loadings. Positive loadings indicate a positive relationship with the component, while negative loadings indicate a negative relationship.

```{r}
barplot(pca$rotation[,1], las=2, col="darkblue")
```

We see that "PerceptionsCorruption" and "NegativeAffect" have a positive effect in PC1 whereas the other variables a negative effect. Therefore, we can interpret this PC1 has the "unhapiness of the countries"

pca\$x[,1] gives you a vector of the PC1 scores for all the data points in your analysis. These scores represent how each observation contributes to PC1 and can be used for various purposes, including ranking or analyzing the data based on this component. Now we can rank the Countries by their PC1 scores: level of unhappiness:

```{r}
# Take the first 5 countries which have a high value of the scores but with NEGATIVE sign which  indicates a negative association with PC1. These countries have less level of unhappiness.
rows_first = order(pca$x[,1])[1:5]
country_names[rows_first]

# Take the first 5 countries which have a high value of the scores but with POSITIVE sign which  indicates a positive association with PC1. These countries have higer level of unhappiness.
rows_last <- tail(order(pca$x[,1]), 5)
country_names[rows_last]

```

\###**2º Component**

We do the same for PC2

```{r}
fviz_contrib(pca, choice = "var", axes = 2)
```

In this case, we see that the contribution of "Score" is really low, so we are not interpreting this component relating it with level of hapiness. Now, we focus on "Generosity" which is the variable that contribute most.

```{r}
barplot(pca$rotation[,2], las=2, col="darkblue")
```

We can see that when PC2 decreases with "Generosity" and "PositiveAffect" while it increases with "LogGDP" and " HLE". We could interpret this as the countries with higher GDP and Healthy Life Expectancy tend to be less generous and have less laugh, enjoyment (positiveAffect)

### **3º Component**

```{r}
fviz_contrib(pca, choice = "var", axes = 3)
```

Here, "PerceptionsCorruption" is now our target.

```{r}
barplot(pca$rotation[,3], las=2, col="darkblue")
```

### Biplot

A biplot displays both the observations (data points) and the variables (original features), allowing you to visualize the relationships and associations between them in a reduced-dimensional space. The variables are represented by arrows that point in the direction of increasing values. The length of these arrows represents the strength of the relationship between the variables and the principal components. Longer arrows indicate stronger relationships.

```{r}
biplot(pca)
```

In this case, it is not very useful because of the high amount of countries

We can make a plot that displays the contributions of each variable to the principal components. Variables that make a stronger contribution to a component will be highlighted in the plot, helping you identify which variables are most influential in shaping the PCA results.

```{r}
fviz_pca_var(pca, col.var = "contrib")
```

As we have said previously, we see we can see a positive contribution of "PerceptionsCorruption" and "NegativeAffect" on PC1 (Dim1), as well as "LogGDP" and "HLE" on PC2 (Dim2).

```{r}
fviz_pca_biplot(pca, repel = TRUE)
```

With repel = TRUE, the labels of observations and variables in the biplot will automatically adjust their positions to avoid overlapping, making it easier to interpret and visualize the relationships between observations and variables in the reduced-dimensional space when we have a large dataset.

## The Scores

Remember, for the $j$-th principal component: $Z_j = X a_j$, $a_j$ denotes the loadings, and $Z_j$ denotes the scores

Let's plot the first two scores, using colors for minutes played:

```{r}
data.frame(z1=-pca$x[,1],z2=pca$x[,2]) %>% 
  ggplot(aes(z1,z2,label=country_names,color=data2$Score)) + geom_point(size=0) +
  labs(title="PCA", x="PC1", y="PC2") +
  theme_bw() + scale_color_gradient(low="lightblue", high="darkblue")+theme(legend.position="bottom") + geom_text(size=2, hjust=0.6, vjust=0, check_overlap = TRUE) 
```

```{r}
data.frame(z1=-pca$x[,1],z2=data2$Score) %>% 
  ggplot(aes(z1,z2,label=country_names,color=data2$LogGDP)) + geom_point(size=0) +
  labs(title="Relationship between PC1 and Score", x="PC1", y="Score") +
  theme_bw() + scale_color_gradient(low="blue", high="darkgreen")+theme(legend.position="bottom") + geom_text(size=2, hjust=0.6, vjust=0, check_overlap = TRUE) 
```

We can see a clear linear relationship

# Factor Analysis

(Based on lab FA)

Factor Analysis is a statistical method used for data reduction and dimensionality reduction. The goal of Factor Analysis is to identify a smaller number of latent factors that can explain the correlations or covariations between the observed variables. It's common to perform factor rotation to make the factors more interpretable. In our case we are using "varimax"

```{r}
fa <- factanal(data2, factors = 4, rotation="varimax", scores="Bartlett", lower = 0.01)
fa
```

When the loading is close to 0, the factor does not explain too much. Therefore, if we look to SS loadings, we see that Factor4 is not very good (0.384).

Let's try to perform Factor Analysis with 3 factors

```{r}
fa2 <- factanal(data2, factors = 3, rotation="varimax", scores="Bartlett", lower = 0.01)
fa2
```

Now, all factor SS loadings are \> 1, and we can say that they explain better If we look at the Cumulative Var, we notice that the 3 factors explain 65.6% of the Variance, which is actually not really good.

## Interpretation

Now, we create a dataframe that combines the factor loadings and uniquenesses into a single data frame. Factor loading is a measure of the relationship between each observed variable and the extracted factors. High factor loadings indicate a strong relationship between the variable and the factor.

Communality represents the proportion of variance in an observed variable that is explained by the retained factors. High communality values suggest that the factor model explains a large portion of the variable's variance. The uniqueness is complementary to communality (= 1-Communality) and it refers to the proportion of variance in an observed variable that is unique or specific to that variable and not explained by the underlying factors extracted by the analysis.

```{r}
cbind(fa2$loadings, fa2$uniquenesses)

```

```{r}
par(mfrow=c(3,1))
barplot(fa2$loadings[,1], names=F, las=2, col="darkblue", ylim = c(-1, 1))
barplot(fa2$loadings[,2], names=F, las=2, col="darkblue", ylim = c(-1, 1))
barplot(fa2$loadings[,3], las=2, col="darkblue", ylim = c(-1, 1))
```

Using the previous plots and dataframe we can associate to the original variavles to the factors Factor1: "Score", "LogGDP", "SocialSupport", "HLE" and negatively "NegativeAffect" Factor2: Negative "PerceptionsCorruption" Factor3: "Freedom", "Generosity", "PositiveAffect"

With this, we can interpret: - Factor 1 appears to represent a factor related to overall well-being, happiness, and life quality. - Factor 2 appears to represent a factor related to the absence of perceptions of corruption. - Factor 3 appears to represent a factor related to positive social attributes or attitudes.

In addition to that if we look at uniqueness, we see that "SocialSupport" has the lowest value: 0.01 therefore its variance is really well explained by the factors. On the other hand, "Generosity" has the highest value: 0.834, which means that the factors don't make a good job explaing its variance.

## Scores

Factor scores are values that represent the degree to which each observation in a dataset exhibits certain characteristics or behaviors associated with underlying latent factors.

```{r}

factor.df <- data.frame(Score_hapiness = data2$Score, fa2$scores)

# Reshape the data to long format using gather
factor.df <- factor.df %>% 
  gather("factor", "score", -Score_hapiness)

# Create the ggplot visualization
factor.df %>%
  ggplot(aes(x = Score_hapiness, y = score, color = factor)) +
  geom_line(size = 1) +
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_color_brewer(palette = "Dark2") +
  facet_wrap(~factor, ncol = 1) +
  labs(title = "3-factor model", x = "Score_hapiness", y = "Scores", col = "")
```

Factor 1 is associated with well-being. We see a positive trend for Factor 1, this suggests that higher "Score_happiness" values are associated with higher well-being.

Factor 2 is associated absence of perceptions of corruption. We see a positive trend for Factor 2, this suggests that higher "Score_happiness" values are associated no perception of corruption.

Factor 3 is associated with positive social attributes. We see a positive trend for Factor 3, this suggests that higher "Score_happiness" values are associated with higher positive social attributes.

# Clustering

(Based on labs of clustering)

Clustering is a machine learning and data analysis technique that involves grouping similar data points or objects together based on certain characteristics or features. The goal of clustering is to discover underlying patterns or structures in data, which can be helpful for various purposes, such as data exploration, pattern recognition, and decision-making.

In our case, clustering can help segment countries into distinct groups based on their happiness-related characteristics.

## K-Means

We are going to try the number of clusters as geographical regions as they might naturally lead to clusters. It could be a possibility that countries within the same continent tend to have similar happiness profiles. However, we will compute the number of clusters in a more sophiticated way later.

Here, we apply the K-Means clustering algorithm to our data. We scales the data. centers=k specifies the number of clusters, and nstart=1000 sets the number of times the algorithm will attempt to find the optimal cluster centers. The results are stored in the fit object. "groups" indicates which cluster each data point belongs to.

Then, we create a bar plot showing the distribution of data points in each cluster. The table(groups) function counts the number of data points in each cluster,

```{r}

set.seed(123) # In order to obtain the same clusters for explanations
k = 5
# nstart: make the random process nstart times, 
fit = kmeans(scale(data2), centers=k, nstart=1000)
groups = fit$cluster

barplot(table(groups), col="blue")
# Heuristic because we select the first centroids randomly
```

We notice an imbalance in the sizes of the clusters

### Interpretation of centers

1)  We are visualizing the cluster centers, which represent the mean values of the observations within each cluster. In order to compare and analyze the cluters it is better to plot them together.

```{r}

centers=fit$centers
# Centers are the mean value of the observations in the group. It contains the mean for each feature
par(mfrow=c(2,3))

barplot(centers[1,], las=2, col="lightblue", main = "Cluster 1")
barplot(centers[2,], las=2, col="lightblue",main = "Cluster 2")
barplot(centers[3,], las=2, col="lightblue", main = "Cluster 3")
barplot(centers[4,], las=2, col="lightblue", main = "Cluster 4")
barplot(centers[5,], las=2, col="lightblue", main = "Cluster 5")

# Difficult to understand as I could not say in which group is Spain
```

These visualizations can provide insights into how each cluster differs in terms of the feature values. We can compare the bar plots to understand the characteristics and differences of the clusters. However, it is not very easy to understand as we cannot see which cluster corresponds to each country.

(NOOO: We can divide our 5 clusters in 2 types:- "NegativeAffect" and "PerceptionsCorruption" affect positively: Cluster1: )

### Clusplot

For this reason, we make a plot to visualize our clustered data in a 2D PCA space, with points representing countries and colors indicating the clusters.

```{r}
# clusplot
#options(repr.plot.width = 15, repr.plot.height = 6)


fviz_cluster(fit, data = data2, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=country_names,hjust=0, vjust=0,size=2,check_overlap = F)+scale_fill_brewer(palette="Paired") 

```

```{r}
# Let's make another plot with check_overlap = T, which is not going to display all countries, but it will more understandable

fviz_cluster(fit, data = data2, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=country_names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired") 

```

```{r}

# Make a little zoom
fviz_cluster(fit, data = data2, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=country_names,hjust=0, vjust=0,size=2,check_overlap = F)+scale_fill_brewer(palette="Paired")  + coord_cartesian(xlim = c(-5, 9), ylim = c(-5, 4))
```

We need to remember the interpretation of the PCs. PC1 represent the "unhapiness of the countries" PC2 decreases with "Generosity" and "PositiveAffect" while it increases with "LogGDP" and " HLE".

In cluster 5, the red one, all variables positively contribute to the cluster except from "NegativeAffect" and "PerceptionsCorruption". So, in this cluster we have the happiest and with better situation countries as Finland.

On the other hand, cluster 4 is the oppostite, as all variables negatively contribute to the cluster except from "NegativeAffect" and "PerceptionsCorruption". So, in this cluster we have the unhappiest and with worse situation countries as Afghanistan

An alternative approach is to leverage the eclust function provided by the factoextra package. This function offers a wide range of clustering methods (such as k-means, pam, hierarchical clustering, and more), as well as various distance metrics and linkage methods to suit your specific clustering needs.

```{r}
data_name = data2
rownames(data_name) <- country_names
fit.kmeans <- eclust(data_name, "kmeans", stand=TRUE, k=5)

```

### Silhouette plot

The silhouette plot is a graphical representation of the silhouette coefficients for each data point in the clusters ( measure of how similar a data point is to its own cluster compared to other clusters). It can help you assess the quality of the clustering by visualizing how well-separated and consistent the clusters are.

A high silhouette coefficient suggests a good cluster assignment. A silhouette coefficient near 0 suggests that the data point is on or very close to the decision boundary between two neighboring clusters. A negative silhouette coefficient indicates that the data point may have been assigned to the wrong cluster.

```{r}
# Distances between the points and the clusters
d <- dist(scale(data2), method="euclidean")  
sil = silhouette(groups, d)
plot(sil, col=1:5, main="", border=NA)
```

```{r}
summary(sil)
```

The cluster with the highest average silhouette width (0.44 C5) is the most well-separated and cohesive, while the cluster with the lowest average silhouette width (0.15 C1) is relatively less well-separated. The clusters with smaller average silhouette width (closer to 0) suggests that the data points within the cluster may be closer to the decision boundary between clusters.

```{r}
# the same with factoextra
fviz_silhouette(fit.kmeans)
```

The negative values (close to -1) means that a data point is more similar to a neighboring cluster than its own, suggesting a potential misclassification.

### Number of clusters

The choice of the number of clusters (k) is a crucial decision in clustering analysis, as it significantly impacts the quality and interpretability of the results.

The function **fviz_nbclust** will generate a plot or print a table showing the silhouette score for a range of values of k (the number of clusters). The optimal number of clusters is often associated with the peak or plateau in the silhouette score, where the clusters are well-separated and internally cohesive.

```{r}
fviz_nbclust(scale(data2), kmeans, method = 'silhouette', k.max = 20, nstart = 1000)
```

(2 clusters)

Within-Cluster Sum of Squares (WSS) criterion. The WSS measures the total variation within each cluster, and finding the "elbow point" in the plot of the WSS can help determine the optimal number of clusters.

```{r}
fviz_nbclust(scale(data2), kmeans, method = 'wss', k.max = 20, nstart = 1000)
```

(2 or 3 clusters)

Gap Statistic and the reference distribution for a range of values of k (the number of clusters). The optimal number of clusters is often associated with the value of k that maximizes the Gap Statistic while taking into account the reference distribution.

```{r}
fviz_nbclust(scale(data2), kmeans, method = 'gap_stat', k.max = 10, nstart = 100, nboot = 500)
```

Maybe 2 or 3 clusters

Perform clustering with the best number of clusters. k = 2

```{r}
fit2 = kmeans(data2, centers = 2, nstart = 25)

# Is the classification more balanced than before?
groups2 = fit2$cluster
barplot(table(groups2), col="#ADD8E6")


```

```{r}
# The clusplot can also be checked, observing that, indeed, a more balanced
# classification is obtained with the number of clusters equal to 2.
fviz_cluster(fit2, data = data2, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=country_names,hjust=0, vjust=0,size=2,check_overlap = F)+scale_fill_brewer(palette="Paired") 

```

## PAM

Partitioning Around Medoids. The key difference between PAM and k-means is that PAM uses medoids as cluster centers, which are actual data points from the dataset (countries in our case), rather than the means (centroids) as in k-means. This makes PAM more robust to outliers and noise in the data.

```{r}
fit.pam <- eclust(data2, "pam", stand=TRUE, k=5, graph=F)

fviz_cluster(fit.pam, data = data2, geom = c("point"), pointsize=1)+
  theme_minimal()+geom_text(label=country_names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")
```

We obtain different cluster that the ones from k-means

### Number clusters PAM

```{r}
fviz_nbclust(scale(data2), pam, method = 'silhouette', k.max = 20, nstart = 100)
```

(2 clusters)

```{r}
fviz_nbclust(scale(data2), pam, method = 'wss', k.max = 20, nstart = 100)
```

(2 or 3 clusters)

Perform clustering with the best number of clusters. k = 2 PAM

```{r}
fit.pam2 <- eclust(data2, "pam", stand=TRUE, k=2, graph=F)

fviz_cluster(fit.pam2, data = data2, geom = c("point"), pointsize=1)+
  theme_minimal()+geom_text(label=country_names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")
```

#### Similarity between clusters PAM - k-means

```{r}
# Computes the adjusted Rand index comparing two classifications.
# The closer to 1 the more agreement

# Similarity between for k = 5
adjustedRandIndex(fit.kmeans$cluster, fit.pam$clustering) 

# Similarity between for k = 2
adjustedRandIndex(fit2$cluster, fit.pam2$clustering) 
```

They quite similar

### Maps for clustering

```{r}

# Select here your favorite clustering tool
map = data.frame(country=country_names, value=fit.pam$clustering)
#map = data.frame(country=names, value=fit.kmeans$cluster)

#Convert the country code into iso3c using the function countrycode()
map$country = countrycode(map$country, 'country.name', 'iso3c')
#Create data object supporting the map
matched <- joinCountryData2Map(map, joinCode = "ISO3",
                               nameJoinColumn = "country")
```

```{r}
#Draw the map
mapCountryData(matched,nameColumnToPlot="value",missingCountryCol = "white",
               borderCol = "#C7D9FF",
               catMethod = "pretty", colourPalette = "topo",
               mapTitle = c("Clusters"), lwd=1)
```

## Kernel k-means

Kernel k-means is a variant of the traditional k-means clustering algorithm that uses kernel functions to perform clustering in a high-dimensional feature space. It is particularly useful when dealing with data that is not linearly separable in the original feature space. Kernel k-means allows for non-linear separation of clusters by implicitly mapping the data into a higher-dimensional space where linear separation may be possible.

```{r}

fit.ker <- kkmeans(as.matrix(data2), centers=5, kernel="rbfdot") # Radial Basis kernel (Gaussian)
```

Retrieve the cluster centers from the kernel k-means clustering results

```{r}
centers(fit.ker)
```

Obtain the number of data points in each cluster,

```{r}
size(fit.ker)
```

We see again unbalanced clusters

Obtain a WSS (within-cluster sum of squares for each cluster) for each cluster in the kernel k-means. Lower WSS values indicate that the data points within a cluster are closer to the cluster center, suggesting a more compact cluster.

```{r}
withinss(fit.ker)
```

```{r}
object.ker = list(data = data2, cluster = fit.ker@.Data)
fviz_cluster(object.ker, geom = c("point"), ellipse=F,pointsize=1)+
  theme_minimal()+geom_text(label=country_names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")
```

Again, we obtain very different clusters

## Hierarchical clustering

```{r}
d = dist(scale(data2), method = "euclidean")
hc <- hclust(d, method = "ward.D2") 
```

### Plots

\####**Dendogram**

```{r}
hc$labels <- country_names

fviz_dend(x = hc, 
          k=5,
          palette = "jco", 
          rect = TRUE, rect_fill = TRUE, 
          rect_border = "jco"  )        
```

It is not clear as we have a lot of observations (countries)

\####**Geographical map**

```{r}
groups.hc = cutree(hc, k = 5)

# Map our PCA index in a map:
map = data.frame(country=country_names, value=groups.hc)
#Convert the country code into iso3c using the function countrycode()
map$country = countrycode(map$country, 'country.name', 'iso3c')
#Create data object supporting the map
matched <- joinCountryData2Map(map, joinCode = "ISO3",
                               nameJoinColumn = "country")



```

```{r}
#Draw the map

mapCountryData(matched,nameColumnToPlot="value",missingCountryCol = "white",
               borderCol = "#C7D9FF",
               catMethod = "pretty", colourPalette = "topo",
               mapTitle = c("Clusters"), lwd=1)

```

# Heatmaps

The heatmap provides a visual representation of the similarities and differences between the rows and columns of your data. The color intensity in the heatmap represents the values of your scaled data, with darker colors indicating higher values.

```{r}

heatmap(scale(data_name), scale = "none",
        distfun = function(x){dist(x, method = "euclidean")},
        hclustfun = function(x){hclust(x, method = "ward.D2")},
        cexRow = 0.7)
```

We see that we have 2 main groups of countries, Group 1 which has high values for "PerceptionsCorruption" and "NegativeAffect". In this group we have countries with lower happiness as Iraq.

Group 2 with high values in the other variables. We have countries with higher happines as Spain and New Zealand.

```{r}
write.table(data2, "data_final.xls", sep=",")
```

# CONCLUSIONS

Throughout this project, I have gained valuable insights into the significance of data cleaning techniques. By addressing issues such as missing values, duplicates, and irrelevant data points, I have ensured the data's quality and reliability for subsequent analysis.

The utilization of dimensionality reduction methods, specifically Principal Component Analysis (PCA) and Factor Analysis (FA), has provided a deeper understanding of our dataset. These techniques have enabled me to uncover the relationships between variables, emphasizing the strong positive correlation between happiness levels and factors like GDP and Social Support, while revealing negative associations with Perceptions of Corruption and Negative Affect.

Clustering methodologies, including k-means, PAM, and kernel k-means, have proven instrumental in categorizing countries based on their unique characteristics. By forming these clusters, I have identified commonalities and disparities among countries, offering valuable insights into the potential drivers of happiness.

A notable outcome of this analysis is the identification of two primary country groups characterized by their "Perceptions of Corruption" and "Negative Affect" values. These groupings correspond to variations in happiness levels and have provided valuable geographical insights into the distribution of happiness across countries.

In summary, this project has highlighted the significance of data cleaning, dimensionality reduction, and clustering techniques in uncovering patterns and associations within the World Happiness dataset. These findings contribute to a deeper understanding of the factors influencing happiness and the geographic distribution of well-being, facilitating informed decision-making and further research in this area.
